{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FishSchoolSeach_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOthVFR9to4VB9d8uHZFg2n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imenFerjani/Road-Anomalies-Detection/blob/main/FishSchoolSeach_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WS_zqrfG-r_P"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZ0JCTzwhHxH",
        "outputId": "43701b92-87b1-4c2d-bea5-db393fab2052",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "################# Import Packages ###########################\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import keras\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "plt.style.use('seaborn')\n",
        "from copy import deepcopy,copy\n",
        "import pandas as pd\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "import random\n",
        "from __future__ import print_function\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Embedding, SpatialDropout1D, Conv1D, MaxPool1D, Flatten, concatenate, Dense, Dropout\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.datasets import imdb\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer, one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrzaShCJy52n",
        "outputId": "9ab7e5c6-a806-4ebb-97c6-16e079fcaa11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bKW3hThzAeS"
      },
      "source": [
        "# WORD-level classification\n",
        "MAX_NUM_WORDS     = 5000\n",
        "EMBEDDING_DIM     = 300\n",
        "MAX_SEQ_LENGTH    = 512\n",
        "\n",
        "# GENERAL\n",
        "DROPOUT_RATE      = 0.5\n",
        "#NB_CLASSES        = 1\n",
        "\n",
        "# LEARNING\n",
        "BATCH_SIZE        = 32\n",
        "NB_EPOCHS         = 1\n",
        "#RUNS              = 5\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPPZNHkylB0H",
        "outputId": "fc2af9b3-b240-49d8-a62b-a27b3055a4f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "# loading imdb database\n",
        "print('Loading data...')\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=MAX_NUM_WORDS)\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')\n",
        "\n",
        "\n",
        "print('Pad sequences (samples x time)')\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=MAX_SEQ_LENGTH)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=MAX_SEQ_LENGTH)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "print('Pad sequences (samples x time)')\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=MAX_SEQ_LENGTH)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=MAX_SEQ_LENGTH)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "25000 train sequences\n",
            "25000 test sequences\n",
            "Pad sequences (samples x time)\n",
            "x_train shape: (25000, 500)\n",
            "x_test shape: (25000, 500)\n",
            "Pad sequences (samples x time)\n",
            "x_train shape: (25000, 500)\n",
            "x_test shape: (25000, 500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDYDJ3M3mReF",
        "outputId": "c91332d5-254e-4897-8b82-dae52ce92681",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "print(x_train.shape,x_test.shape,y_train.shape,y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-08fa712edb3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPMUxW3zwY3z",
        "outputId": "ebf348a8-ce14-4917-d3bc-3c62d8c86d86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(type(x_train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nMLuqDpwN5V"
      },
      "source": [
        "x_train=np.concatenate((x_train , x_test[:10000,:]))\n",
        "x_test = x_test[10000:,:]\n",
        "y_train=np.concatenate((y_train , y_test[:10000]))\n",
        "y_test = y_test[10000:]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHU0fzwxz8uP",
        "outputId": "7af55649-d545-41da-bf10-a47a1fb902e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "\n",
        "# loading reuters database\n",
        "\n",
        "from keras.datasets import reuters\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = reuters.load_data(path=\"reuters.npz\",\n",
        "                                                         num_words=MAX_NUM_WORDS,\n",
        "                                                         skip_top=0,\n",
        "                                                         maxlen=MAX_SEQ_LENGTH,\n",
        "                                                         test_split=0.2,\n",
        "                                                         seed=113,\n",
        "                                                         start_char=1,\n",
        "                                                         oov_char=2,\n",
        "                                                         index_from=3)\n",
        "word_index = reuters.get_word_index(path=\"reuters_word_index.json\")\n",
        "\n",
        "print('# of Training Samples: {}'.format(len(x_train)))\n",
        "print('# of Test Samples: {}'.format(len(x_test)))\n",
        "\n",
        "nb_classes = max(y_train) + 1\n",
        "print('# of Classes: {}'.format(nb_classes))\n",
        "index_to_word = {}\n",
        "for key, value in word_index.items():\n",
        "    index_to_word[value] = key\n",
        "print(' '.join([index_to_word[x] for x in x_train[0]]))\n",
        "print(y_train[0])\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
        "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
        "x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, nb_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, nb_classes)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/reuters_word_index.json\n",
            "557056/550378 [==============================] - 0s 0us/step\n",
            "# of Training Samples: 8384\n",
            "# of Test Samples: 2097\n",
            "# of Classes: 46\n",
            "the of of mln loss for plc said at only ended said of could 1 traders now april 0 a after said from 1985 and from foreign 000 april 0 prices its account year a but in this mln home an states earlier and rise and revs vs 000 its 16 vs 000 a but 3 of of several and shareholders and dividend vs 000 its all 4 vs 000 1 mln agreed of april 0 are 2 states will billion total and against 000 pct dlrs\n",
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sjw1WiNDhqpj"
      },
      "source": [
        "# loading Yelp database\n",
        "filepath_dict = {'yelp':'yelp_labelled.txt','amazon':'amazon_cells_labelled.txt','imdb':'imdb_labelled.txt'}\n",
        "df_list = []\n",
        "for source, filepath in filepath_dict.items():\n",
        "    df = pd.read_csv(filepath, names=['sentence', 'label'], sep='\\t')\n",
        "    df['source'] = source  # Add another column filled with the source name\n",
        "    df_list.append(df)\n",
        "df = pd.concat(df_list)\n",
        "df_yelp = df[df['source'] == 'yelp']\n",
        "sentences = df_yelp['sentence'].values\n",
        "y = df_yelp['label'].values\n",
        "sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.25, random_state=1000)\n",
        "encoder = LabelEncoder()\n",
        "y_train= encoder.fit_transform(y_train)\n",
        "y_test=encoder.fit_transform(y_test)\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
        "tokenizer.fit_on_texts(sentences_train)\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(sentences_train)\n",
        "X_test = tokenizer.texts_to_sequences(sentences_test)\n",
        "\n",
        "#vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
        "\n",
        "maxlen = MAX_SEQ_LENGTH #100\n",
        "\n",
        "x_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "x_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKXQzKkI_Ip6"
      },
      "source": [
        "# loading amazon database\n",
        "\n",
        "filepath_dict = {'yelp':'yelp_labelled.txt','amazon':'amazon_cells_labelled.txt','imdb':'imdb_labelled.txt'}\n",
        "df_list = []\n",
        "for source, filepath in filepath_dict.items():\n",
        "    df = pd.read_csv(filepath, names=['sentence', 'label'], sep='\\t')\n",
        "    df['source'] = source  # Add another column filled with the source name\n",
        "    df_list.append(df)\n",
        "df = pd.concat(df_list)\n",
        "df_yelp = df[df['source'] == 'amazon']\n",
        "sentences = df_yelp['sentence'].values\n",
        "y = df_yelp['label'].values\n",
        "sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.25, random_state=1000)\n",
        "encoder = LabelEncoder()\n",
        "y_train= encoder.fit_transform(y_train)\n",
        "y_test=encoder.fit_transform(y_test)\n",
        "#vectorizer = CountVectorizer()\n",
        "#vectorizer.fit(sentences_train)\n",
        "\n",
        "#X_train = vectorizer.transform(sentences_train)\n",
        "#X_test  = vectorizer.transform(sentences_test)\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
        "tokenizer.fit_on_texts(sentences_train)\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(sentences_train)\n",
        "X_test = tokenizer.texts_to_sequences(sentences_test)\n",
        "\n",
        "#vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
        "\n",
        "maxlen = MAX_SEQ_LENGTH #100\n",
        "\n",
        "x_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "x_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSwbWYD1-UhR",
        "outputId": "91456d91-dc0a-4d92-a6e7-c4827a3f4122",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Yahoo Answers Dataset DATASET\n",
        "train_data=pd.read_csv('trainyahoo.csv',header=None, names=['class_id','qtitle','question','best_answer'], error_bad_lines=False, engine='python', encoding='utf8')\n",
        "test_data=pd.read_csv(\"testyahoo.csv\",header=0, names=['class_id','qtitle','question','best_answer'], error_bad_lines=False, engine='python', encoding='utf8')\n",
        "frames = [train_data, test_data]\n",
        "dataset_agnews_fss = pd.concat(frames)\n",
        "c1=dataset_agnews_fss.query('class_id == 1').sample(1000)\n",
        "c2=dataset_agnews_fss.query('class_id == 2').sample(1000)\n",
        "c3=dataset_agnews_fss.query('class_id == 3').sample(1000)\n",
        "c4=dataset_agnews_fss.query('class_id == 4').sample(1000)\n",
        "c5=dataset_agnews_fss.query('class_id == 5').sample(1000)\n",
        "c6=dataset_agnews_fss.query('class_id == 6').sample(1000)\n",
        "c7=dataset_agnews_fss.query('class_id == 7').sample(1000)\n",
        "c8=dataset_agnews_fss.query('class_id == 8').sample(1000)\n",
        "c9=dataset_agnews_fss.query('class_id == 9').sample(1000)\n",
        "c10=dataset_agnews_fss.query('class_id == 10').sample(1000)\n",
        "dataset_agnews_fss=pd.concat([c1,c2,c3,c4,c5,c6,c7,c8,c9,c10])\n",
        "dataset_agnews_fss.best_answer=dataset_agnews_fss.best_answer.astype(str)\n",
        "sentences_fss = dataset_agnews_fss['best_answer'].values\n",
        "y_fss = dataset_agnews_fss['class_id'].values\n",
        "\n",
        "sentences_train_fss, sentences_test_fss, y_train_fss, y_test_fss =train_test_split(sentences_fss, y_fss, test_size=0.25, random_state=1000)\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
        "tokenizer.fit_on_texts(sentences_train_fss)\n",
        "\n",
        "X_train_fss = tokenizer.texts_to_sequences(sentences_train_fss)\n",
        "X_test_fss = tokenizer.texts_to_sequences(sentences_test_fss)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
        "maxlen = MAX_SEQ_LENGTH\n",
        "\n",
        "X_train_fss = pad_sequences(X_train_fss, padding='post', maxlen=maxlen)\n",
        "X_test_fss = pad_sequences(X_test_fss, padding='post', maxlen=maxlen)\n",
        "\n",
        "y_train_fss=[x-1 for x in y_train_fss]\n",
        "y_test_fss=[x-1 for x in y_test_fss]\n",
        "y_train_fss = keras.utils.to_categorical(y_train_fss, num_classes=10)\n",
        "y_test_fss = keras.utils.to_categorical(y_test_fss, num_classes=10)\n",
        "\n",
        "x_train,x_test,y_train,y_test = X_train_fss[:1000,:], X_test_fss[:1000,:], y_train_fss[:1000,:],y_test_fss[:1000,:]\n",
        "#x_train,x_test,y_train,y_test = X_train_fss[:1000,:], X_test_fss[:1000,:], y_train_fss[:1000,:],y_test_fss[:1000,:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Skipping line 1063032: unexpected end of data\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "er0P3kSON6cD",
        "outputId": "0d435dd6-8027-4f92-95ab-13e0bc0dc2e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(sentences_fss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_QdWRBrC3m4"
      },
      "source": [
        "# loading Dataset\n",
        "# word preprocessing dataset\n",
        "\n",
        "train_data=pd.read_csv('/gdrive/My Drive/textdatasets/trainyahoo.csv',header=None, names=['classid','a','b','desc'], error_bad_lines=False, engine='python', encoding='utf8')\n",
        "test_data=pd.read_csv('/gdrive/My Drive/textdatasets/testyahoo.csv',header=0, names=['classid','a','b','desc'], error_bad_lines=False, engine='python', encoding='utf8')\n",
        "train_data.desc = train_data.desc.astype(str)\n",
        "test_data.desc = test_data.desc.astype(str)\n",
        "#train_data=pd.read_csv('train.csv', delimiter=',',header=0,names=['classid','title','desc'])\n",
        "#test_data=pd.read_csv(\"test.csv\",header=0,names=['classid','title','desc'])\n",
        "frames = [train_data, test_data]\n",
        "\n",
        "dataset_agnews_fss = pd.concat(frames)\n",
        "#dataset_agnews_fss = dataset_agnews[:1000]\n",
        "sentences_fss = dataset_agnews_fss['desc'].values\n",
        "y_fss = dataset_agnews_fss['classid'].values\n",
        "\n",
        "sentences_train_fss, sentences_test_fss, y_train_fss, y_test_fss =train_test_split(sentences_fss, y_fss, test_size=0.0428571429, random_state=1000)\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
        "tokenizer.fit_on_texts(sentences_train_fss)\n",
        "\n",
        "X_train_fss = tokenizer.texts_to_sequences(sentences_train_fss)\n",
        "X_test_fss = tokenizer.texts_to_sequences(sentences_test_fss)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
        "maxlen = MAX_SEQ_LENGTH\n",
        "\n",
        "X_train_fss = pad_sequences(X_train_fss, padding='post', maxlen=maxlen)\n",
        "X_test_fss = pad_sequences(X_test_fss, padding='post', maxlen=maxlen)\n",
        "\n",
        "y_train_fss=[x-1 for x in y_train_fss]\n",
        "y_test_fss=[x-1 for x in y_test_fss]\n",
        "y_train_fss = keras.utils.to_categorical(y_train_fss, num_classes=10)\n",
        "y_test_fss = keras.utils.to_categorical(y_test_fss, num_classes=10)\n",
        "\n",
        "x_train,x_test,y_train,y_test = X_train_fss[:1000,:], X_test_fss[:1000,:], y_train_fss[:1000,:],y_test_fss[:1000,:]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCfoABsniCox"
      },
      "source": [
        "#Yelp FULL DATASET################################################################\n",
        "train_data=pd.read_csv('trainamzF.csv',header=None, names=['class_id','title','desc'], error_bad_lines=False, engine='python', encoding='utf8')\n",
        "test_data=pd.read_csv(\"testamzF.csv\",header=0, names=['class_id','title','desc'], error_bad_lines=False, engine='python', encoding='utf8')\n",
        "\n",
        "frames = [train_data, test_data]\n",
        "\n",
        "dataset_agnews_fss = pd.concat(frames)\n",
        "#dataset_agnews_fss = dataset_agnews[:1000]\n",
        "sentences_fss = dataset_agnews_fss['desc'].values\n",
        "y_fss = dataset_agnews_fss['class_id'].values\n",
        "\n",
        "sentences_train_fss, sentences_test_fss, y_train_fss, y_test_fss =train_test_split(sentences_fss, y_fss, test_size=0.25, random_state=1000)\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
        "tokenizer.fit_on_texts(sentences_train_fss)\n",
        "\n",
        "X_train_fss = tokenizer.texts_to_sequences(sentences_train_fss)\n",
        "X_test_fss = tokenizer.texts_to_sequences(sentences_test_fss)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
        "maxlen = MAX_SEQ_LENGTH\n",
        "\n",
        "X_train_fss = pad_sequences(X_train_fss, padding='post', maxlen=maxlen)\n",
        "X_test_fss = pad_sequences(X_test_fss, padding='post', maxlen=maxlen)\n",
        "\n",
        "y_train_fss=[x-1 for x in y_train_fss]\n",
        "y_test_fss=[x-1 for x in y_test_fss]\n",
        "y_train_fss = keras.utils.to_categorical(y_train_fss, num_classes=5)\n",
        "y_test_fss = keras.utils.to_categorical(y_test_fss, num_classes=5)\n",
        "\n",
        "x_train,x_test,y_train,y_test = X_train_fss[:1000,:], X_test_fss[:1000,:], y_train_fss[:1000,:],y_test_fss[:1000,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TScY9KC_c6Iz"
      },
      "source": [
        "#Yelp Full DATASET\n",
        "train_data=pd.read_csv('trainyelpF.csv',header=None, names=['class_id','desc'], error_bad_lines=False, engine='python', encoding='utf8')\n",
        "test_data=pd.read_csv(\"testyelpF.csv\",header=0, names=['class_id','desc'], error_bad_lines=False, engine='python', encoding='utf8')\n",
        "frames = [train_data, test_data]\n",
        "dataset_agnews_fss = pd.concat(frames)\n",
        "\n",
        "sentences_fss = dataset_agnews_fss['desc'].values\n",
        "y_fss = dataset_agnews_fss['class_id'].values\n",
        "\n",
        "sentences_train_fss, sentences_test_fss, y_train_fss, y_test_fss =train_test_split(sentences_fss, y_fss, test_size=0.25, random_state=1000)\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
        "tokenizer.fit_on_texts(sentences_train_fss)\n",
        "\n",
        "X_train_fss = tokenizer.texts_to_sequences(sentences_train_fss)\n",
        "X_test_fss = tokenizer.texts_to_sequences(sentences_test_fss)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
        "maxlen = MAX_SEQ_LENGTH\n",
        "\n",
        "X_train_fss = pad_sequences(X_train_fss, padding='post', maxlen=maxlen)\n",
        "X_test_fss = pad_sequences(X_test_fss, padding='post', maxlen=maxlen)\n",
        "\n",
        "y_train_fss=[x-1 for x in y_train_fss]\n",
        "y_test_fss=[x-1 for x in y_test_fss]\n",
        "y_train_fss = keras.utils.to_categorical(y_train_fss, num_classes=5)\n",
        "y_test_fss = keras.utils.to_categorical(y_test_fss, num_classes=5)\n",
        "\n",
        "x_train,x_test,y_train,y_test = X_train_fss[:1000,:], X_test_fss[:1000,:], y_train_fss[:1000,:],y_test_fss[:1000,:]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXsNFaKBOUP1",
        "outputId": "09eac5e2-1038-4147-dcb0-78a2a430d8be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print('x_train shape:', x_train.shape,y_train.shape)\n",
        "print('x_test shape:', x_test.shape,y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (1000, 500) (1000, 5)\n",
            "x_test shape: (1000, 500) (1000, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbLuCOxMlml9"
      },
      "source": [
        "def add_convpool_block(layers, max_out_ch, max_kernel_size):\n",
        "    filters_lenghts = random.choice([3,4])\n",
        "    deb = random.choice([2,3,4,5,6,7])\n",
        "    conv_kernel = random.choice([[i for i in range(deb, deb+filters_lenghts)], [deb for i in range(filters_lenghts)]])\n",
        "    \n",
        "    out_channel = random.choice([64,128,512])\n",
        "    #np.random.randint(128, max_out_ch)\n",
        "    #conv_kernel = np.random.randint(3, max_kernel_size)\n",
        "\n",
        "    layers.append({\"type\": \"convpool_block\", \"ou_c\": out_channel, \"kernel\": conv_kernel})\n",
        "\n",
        "    return layers\n",
        "\n",
        "def add_fc(layers, max_fc_neurons):\n",
        "    layers.append({\"type\": \"fc\", \"ou_c\": np.random.randint(100, max_fc_neurons), \"kernel\": -1})\n",
        "    \n",
        "    return layers\n",
        "\n",
        "def model_compile(list_layers,nb_classes,drop_rate,max_num_words, embedding_dim,max_seq_length):\n",
        "  input_text = Input(shape=(max_seq_length,))\n",
        "  embedding_layer = Embedding(max_num_words, embedding_dim, input_length=max_seq_length)(input_text)\n",
        "  text_embed = SpatialDropout1D(0.2)(embedding_layer)\n",
        "  filter_lengths = list_layers[0]['kernel']\n",
        "  conv_layers = []\n",
        "  \n",
        "  for filter_length in filter_lengths:\n",
        "    conv_layer = Conv1D(filters= list_layers[0]['ou_c'], kernel_size=filter_length, padding='valid',\n",
        "                        strides=1, activation='relu')(text_embed)\n",
        "    maxpooling = MaxPool1D(pool_size=max_seq_length - filter_length + 1)(conv_layer)\n",
        "    flatten = Flatten()(maxpooling)\n",
        "    conv_layers.append(flatten)\n",
        "  sentence_embed = concatenate(inputs=conv_layers)\n",
        "  sentence_embed = Dropout(0.5)(sentence_embed)\n",
        "  dense_layer = Dense(list_layers[-1]['ou_c'], activation='relu')(sentence_embed)\n",
        "  output = Dense(nb_classes, activation='softmax')(dense_layer)\n",
        "  model_c = Model(input_text, output)\n",
        "  model_c.compile(loss='categorical_crossentropy', metrics=['acc'], optimizer= 'adam')\n",
        "  return model_c\n",
        "\n",
        "def model_fit(model_t,x_train, y_train, batch_size,nb_epochs, full_training = False):\n",
        "  if full_training==False:\n",
        "    #IMDB\n",
        "    #x_train1, y_train1 = x_train[:1000], y_train[:1000]\n",
        "    #x_test1, y_test1 = x_test[:1000], y_test[:1000]\n",
        "    #x_train = x_train[:round(x_train.shape[0]/4)]\n",
        "    #y_train = y_train[:round(y_train.shape[0]/4)]\n",
        "    #Reuters and AG'News\n",
        "    x_train1, y_train1 = x_train, y_train\n",
        "    x_test1, y_test1 = x_test, y_test\n",
        "    #print(\"ici1\")\n",
        "    #hist = model.fit(x=x_train1, y=y_train1, batch_size=batch_size, epochs=nb_epochs)\n",
        "    #reuters\n",
        "    #hist = model.fit(x_train, y_train, batch_size=batch_size, epochs=nb_epochs, verbose=1, validation_split=0.1)\n",
        "    hist=model_t.fit(x_train1, y_train1,epochs=1,verbose=1,batch_size=batch_size)\n",
        "    #print(\"ici2\")\n",
        "    \n",
        "    return model_t.evaluate(x_test1, y_test1, verbose=True)\n",
        "  else:\n",
        "    model_t.summary()\n",
        "    hist = model_t.fit(x=x_train, y=y_train,batch_size=batch_size, epochs=nb_epochs)\n",
        "    #return model.evaluate(x_test,  y_test, verbose=True)\n",
        "    return model_t.evaluate(X_test_fss,  y_test_fss, verbose=True)\n",
        "\n",
        "  #model.evaluate(x_train, y_train, verbose=True)\n",
        "  \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GspAcIVO0J_5",
        "outputId": "02a1f020-a545-4b69-8825-35104e192f03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import random\n",
        "layers=[]\n",
        "filters_lenghts = random.choice([3,4])\n",
        "deb = random.choice([2,3,4,5,6,7])\n",
        "conv_kernel = random.choice([[i for i in range(deb, deb+filters_lenghts)], [deb for i in range(filters_lenghts)]])\n",
        "layers.append({\"type\": \"convpool_block\", \"ou_c\": 22, \"kernel\": conv_kernel})\n",
        "layers[0]['kernel']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[5, 5, 5]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLXvD0UQhRxU"
      },
      "source": [
        "\n",
        "class Fish:\n",
        "  def __init__(self,max_num_words,embedding_dim,max_seq_length,max_kernel_size,nb_classes,\n",
        "               min_depth, max_depth,conv_pool_prob, fc_prob, max_out_ch,max_fc_neurons,n_iter):\n",
        "    # Fish Attributes\n",
        "    self.delta_pos = np.nan\n",
        "    self.delta_cost = 0.0\n",
        "    self.weight = n_iter / 4.0\n",
        "    #self.cost = np.nan\n",
        "    self.has_improved = False\n",
        "\n",
        "    # Model attributes\n",
        "    self.depth = np.random.randint(min_depth, max_depth)\n",
        "    self.max_num_words = max_num_words\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.max_seq_length = max_seq_length\n",
        "    self.max_kernel_size = max_kernel_size\n",
        "    self.nb_classes = nb_classes\n",
        "    #self.depth = np.random.randint(min_layer, max_layer)\n",
        "\n",
        "    self.conv_pool_prob = conv_pool_prob\n",
        "    self.fc_prob = fc_prob\n",
        "    self.max_fc_neurons = max_fc_neurons\n",
        "    self.max_out_ch = max_out_ch\n",
        "        \n",
        "    self.layers = []\n",
        "    self.nb_convpool_blocks=None\n",
        "    self.nb_fc_layers=None\n",
        "    self.acc = None\n",
        "    self.loss= None\n",
        "    self.model=None\n",
        "\n",
        "    self.initialization()\n",
        "  \n",
        "  def __str__(self):\n",
        "    string = \"\"\n",
        "    for z in range(len(self.layers)):\n",
        "      \n",
        "      if self.layers[z][\"type\"]== \"convpool_block\":\n",
        "        string = string + \"Conv_layer\" + \" || \" + \"MaxPoling_layer\" + \" || \"\n",
        "      else:\n",
        "          string = string +  \"FullyConnected_layer\" + \" || \"\n",
        "\n",
        "    return '\\033[94m'+'\\033[1m'+string+'\\033[0m'\n",
        "\n",
        "    # Build fish (CNN) architecture\n",
        "  def update_layers(self):\n",
        "    nb_convpool_blocks= len([i for i in range(len(list_layers)) if self.layers[i][\"type\"] == \"convpool_block\"])\n",
        "    nb_fc_layers= len([i for i in range(len(list_layers)) if self.layers[i][\"type\"] == \"fc\"])\n",
        "    return nb_convpool_blocks,nb_fc_layers\n",
        "    \n",
        "  def initialization(self):\n",
        "    \n",
        "    self.nb_convpool_blocks = 1\n",
        "    #round((self.depth * self.conv_pool_prob)/2)\n",
        "    print(\"cnvpool=\",self.nb_convpool_blocks)\n",
        "    self.nb_fc_layers = 1\n",
        "    #self.depth - self.nb_convpool_blocks * 2\n",
        "    print(\"fc=\",self.nb_fc_layers)\n",
        "    # First block is always a convolution layer followed by a pooling layer\n",
        "    for i in range(self.nb_convpool_blocks):\n",
        "      self.layers = add_convpool_block(self.layers, self.max_out_ch, self.max_kernel_size)\n",
        "      \n",
        "    for i in range(self.nb_fc_layers):\n",
        "      self.layers = add_fc(self.layers, self.max_fc_neurons)\n",
        "      \n",
        "    # Last layer is always a fully connected layer    \n",
        "    #self.layers[-1] = {\"type\": \"fc\", \"ou_c\": self.nb_classes, \"kernel\": -1}\n",
        "    \n",
        "    print(self.layers)\n",
        "    print(\"ok1\")\n",
        "    self.model= model_compile(self.layers,self.nb_classes,drop_rate,self.max_num_words, self.embedding_dim, self.max_seq_length)\n",
        "    print(\"ok2\")\n",
        "    \n",
        "    self.loss,self.acc= model_fit(self.model,x_train, y_train, batch_size,nb_epochs)\n",
        "    print(\"ok3\")\n",
        "    \n",
        "\n",
        "##### Model methods ####\n",
        "  \n",
        "\n",
        "  def model_delete(self):\n",
        "    # This is used to free up memory during FSS training\n",
        "    del self.model\n",
        "    keras.backend.clear_session()\n",
        "    tf.reset_default_graph()\n",
        "    self.model = None\n",
        "\n",
        "##### FSS methods ####\n",
        "\n",
        "  def move_to(self,nb_convblocks_new, nb_fc_new):\n",
        "    change=False\n",
        "    print(\"Steps:\",nb_convblocks_new, nb_fc_new)\n",
        "    list_layers1=deepcopy(self.layers)\n",
        "    if nb_convblocks_new > 0:\n",
        "      change=True\n",
        "      filters_lenghts = random.choice([3,4])\n",
        "      deb = random.choice([2,3,4,5,6,7])\n",
        "      conv_kernel = random.choice([[i for i in range(deb, deb+filters_lenghts)], [deb for i in range(filters_lenghts)]])\n",
        "    \n",
        "      out_channel = random.choice([64,128,512])\n",
        "      #print(\"add convpool\")\n",
        "      for i in range(nb_convblocks_new):        \n",
        "        list_layers1.insert(0,{\"type\": \"convpool_block\", \"ou_c\": out_channel, \"kernel\": conv_kernel})\n",
        "    elif nb_convblocks_new < 0 and abs(nb_convblocks_new) < self.nb_convpool_blocks:\n",
        "      #print(\"del convpool\")\n",
        "      change=True \n",
        "      for i in range(int(abs(nb_convblocks_new))):\n",
        "        del list_layers1[0]\n",
        "    elif nb_convblocks_new < 0 and self.nb_convpool_blocks > 1:\n",
        "      change=True\n",
        "      del list_layers1[:self.nb_convpool_blocks-1]\n",
        "    else:\n",
        "      pass;      \n",
        "    if nb_fc_new > 0:\n",
        "      #print(\"add fc\")\n",
        "      change=True\n",
        "      for i in range(nb_fc_new):\n",
        "        list_layers1.insert(-1,{\"type\": \"fc\", \"ou_c\": np.random.randint(1, self.max_fc_neurons), \"kernel\": -1})\n",
        "    elif nb_fc_new < 0 and abs(nb_fc_new) < self.nb_fc_layers :\n",
        "      #print(\"del fc\")\n",
        "      change=True\n",
        "      print(\"architecture before modification:\",list_layers1)\n",
        "      for i in range(int(abs(nb_fc_new))):\n",
        "        del list_layers1[-2]\n",
        "    elif nb_fc_new < 0 and self.nb_fc_layers > 1 :\n",
        "      change=True\n",
        "      del list_layers1[len(list_layers1)-self.nb_fc_layers:len(list_layers1)-1]\n",
        "    else:\n",
        "      pass;\n",
        "    return list_layers1,change\n",
        "\n",
        "  def individual_movement(self):\n",
        "    print(\"Starting individual movement\")\n",
        "    #list_layers = self.layers\n",
        "    try_times=1\n",
        "    max_acc = self.acc\n",
        "    print(\"initial fish accuracy:\", max_acc)\n",
        "    for i in range(try_times):\n",
        "      print(\"try number:\",i)\n",
        "      step_x = np.random.randint(-2,3)\n",
        "      \n",
        "      step_y = np.random.randint(-2,3)\n",
        "      if (step_x,step_y != 0,0):\n",
        "        list_layers,change = self.move_to(step_x,step_y)\n",
        "        print(\"compiling model\")\n",
        "        model_ind = model_compile(list_layers,self.nb_classes, drop_rate,\n",
        "                              self.max_num_words, self.embedding_dim, self.max_seq_length)\n",
        "        print(\"training model\")\n",
        "        loss,acc = model_fit(model_ind,x_train, y_train, batch_size,nb_epochs)\n",
        "        print(\"Fish accuracy after modification\",acc)\n",
        "        if acc > max_acc:\n",
        "          print(\"Improvement after modification:\"+'\\033[92m' +\"YES\"+ '\\033[0m')\n",
        "          max_acc=acc\n",
        "          self.delta_cost = acc - self.acc\n",
        "          self.acc=acc\n",
        "        #nb_convpool_blocks,nb_fc_layers = self.update_layers()\n",
        "          self.delta_pos= (step_x ,step_y)\n",
        "          self.layers= list_layers\n",
        "          \n",
        "          self.nb_convpool_blocks = len([x for x in list_layers if x[\"type\"]==\"convpool_block\"]) \n",
        "          self.nb_fc_layers = len([x for x in list_layers if x[\"type\"]==\"fc\"])\n",
        "          self.loss,self.acc = loss,acc\n",
        "          self.model=model_ind\n",
        "          self.has_improved=True\n",
        "        else: print(\"Improvement after modification:\"+'\\033[91m' +\"NO\"+ '\\033[0m')        \n",
        "    return self\n",
        "        \n",
        "  \n",
        "  def collective_movement(self,nb_convpool_blocks,nb_fc_layers):\n",
        "    list_layers,change = self.move_to(nb_convpool_blocks,nb_fc_layers)\n",
        "    if change:\n",
        "      self.model = model_compile(list_layers,self.nb_classes, drop_rate, self.max_num_words, self.embedding_dim, self.max_seq_length)\n",
        "      loss,acc = model_fit(self.model,x_train, y_train, batch_size,nb_epochs)\n",
        "      #ecrire dans le fichier\n",
        "      self.delta_cost= acc - self.acc\n",
        "      if self.delta_cost > 0:\n",
        "        print(\"Improvement after collective movement:\"+'\\033[92m' +\"YES\"+ '\\033[0m')\n",
        "        self.has_improved= True\n",
        "        self.delta_pos= (nb_convpool_blocks ,nb_fc_layers)\n",
        "        self.layers= list_layers\n",
        "        self.nb_convpool_blocks+=nb_convpool_blocks\n",
        "        self.nb_fc_layers += nb_fc_layers\n",
        "        self.loss,self.acc = loss,acc\n",
        "      else:\n",
        "        print(\"Improvement after collective movement:\"+'\\033[91m' +\"NO\"+ '\\033[0m')\n",
        "        self.has_improved=False\n",
        "\n",
        "    #nb_convpool_blocks,nb_fc_layers = update_layers()\n",
        "    \n",
        "    \n",
        "    return self\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3bxTmQrMtLv"
      },
      "source": [
        "class School:\n",
        "  def __init__(self,n_iter,school_size):\n",
        "    self.n_iter = n_iter\n",
        "    self.dim=2\n",
        "    self.school_size = school_size\n",
        "    self.prev_weight_school = 0.0\n",
        "    self.curr_weight_school = 0.0\n",
        "    self.best_fish = None\n",
        "    self.best_fish_acc=0.0\n",
        "    self.best_fish_loss=0.0\n",
        "    self.best_fish_model=None\n",
        "\n",
        "    \n",
        "\n",
        "    self.optimum_cost_tracking_iter = []\n",
        "    self.optimum_cost_tracking_eval = []\n",
        "\n",
        "    self.school = []\n",
        "    for i in range(school_size):\n",
        "      fish = Fish(max_num_words,embedding_dim,max_seq_length,max_kernel_size,nb_classes,min_depth,\n",
        "                  max_depth,conv_pool_prob, fc_prob, max_out_ch,max_fc_neurons,n_iter)\n",
        "      self.school.append(fish)\n",
        "      \n",
        "      self.curr_weight_school += fish.weight\n",
        "      \n",
        "\n",
        "    self.prev_weight_school = self.curr_weight_school\n",
        "    \n",
        "    self.update_best_fish()\n",
        "    \n",
        "     \n",
        "  def update_best_fish(self):\n",
        "    for fish in self.school:\n",
        "      if self.best_fish_acc < fish.acc:\n",
        "        self.best_fish = copy(fish)\n",
        "        self.best_fish_acc = fish.acc\n",
        "        self.best_fish_loss = fish.loss\n",
        "        self.best_fish_model=fish.model\n",
        "    \n",
        "  def max_delta_cost(self):\n",
        "    max_ = 0\n",
        "    for fish in self.school:\n",
        "      if max_ < fish.delta_cost:\n",
        "        max_ = fish.delta_cost\n",
        "    return max_\n",
        "\n",
        "  def total_school_weight(self):\n",
        "    self.prev_weight_school = self.curr_weight_school\n",
        "    self.curr_weight_school = 0.0\n",
        "    for fish in self.school:\n",
        "      self.curr_weight_school += fish.acc\n",
        "\n",
        "  def calculate_barycenter(self):\n",
        "    barycenter = np.zeros((self.dim,) ,dtype=np.int) # 2 : 1 convpool blocks ; 1 fc layers\n",
        "    density = 0.0\n",
        "    for fish in self.school:\n",
        "      density += fish.acc\n",
        "      barycenter[0] += (fish.nb_convpool_blocks * fish.acc)\n",
        "      barycenter[1] += (fish.nb_fc_layers * fish.acc)\n",
        "    for dim in range(self.dim):\n",
        "      barycenter[dim] = barycenter[dim] / density\n",
        "\n",
        "    return barycenter\n",
        "\n",
        "  '''def update_steps(self, curr_iter):\n",
        "        self.curr_step_individual = self.step_individual_init - curr_iter * float(\n",
        "            self.step_individual_init - self.step_individual_final) / self.n_iter\n",
        "\n",
        "        self.curr_step_volitive = self.step_volitive_init - curr_iter * float(\n",
        "            self.step_volitive_init - self.step_volitive_final) / self.n_iter'''\n",
        "  \n",
        "  \n",
        "  def update_steps(self, curr_iter):\n",
        "    self.curr_step_individual = self.step_individual_init - curr_iter * float(\n",
        "    self.step_individual_init - self.step_individual_final) / self.n_iter\n",
        "    self.curr_step_volitive = self.step_volitive_init - curr_iter * float(\n",
        "    self.step_volitive_init - self.step_volitive_final) / self.n_iter\n",
        "  \n",
        "  \n",
        "  def feeding(self):\n",
        "    max_delta_cost = self.max_delta_cost()\n",
        "    for fish in self.school:\n",
        "      if max_delta_cost :\n",
        "        fish.weight += (fish.delta_cost / max_delta_cost)\n",
        "            \n",
        "  '''def collective_instinctive_movement(self):\n",
        "    cost_eval_enhanced = np.zeros((self.dim,), dtype=np.int)\n",
        "    density = 0.0\n",
        "    for fish in self.school:\n",
        "      density += fish.delta_cost\n",
        "      cost_eval_enhanced[0] = cost_eval_enhanced[0] + (fish.nb_convpool_blocks * fish.delta_cost)\n",
        "      cost_eval_enhanced[1] = cost_eval_enhanced[1] + (fish.nb_fc_layers * fish.delta_cost)\n",
        "    for dim in range(self.dim):\n",
        "      if density != 0:\n",
        "        cost_eval_enhanced[dim] = cost_eval_enhanced[dim] / density\n",
        "    for fish in self.school:\n",
        "      nb_convpool_blocks = int(round(cost_eval_enhanced[0]))      \n",
        "      nb_fc_layers = int(round(cost_eval_enhanced[1]))\n",
        "      #if nb_convpool_blocks + nb_fc_blocks > max_depth or nb_convpool_blocks + nb_fc_blocks < min_depth:\n",
        "        #continue;\n",
        "      #else:\n",
        "      print(\"Collective_instinctive_movement:\",nb_convpool_blocks,nb_fc_layers)\n",
        "      if (nb_convpool_blocks,nb_fc_layers) != (0,0):\n",
        "        fish.collective_movement(nb_convpool_blocks,nb_fc_layers)\n",
        "      else:\n",
        "        print(\"null instinctive \")'''\n",
        "  def collective_movement(self):\n",
        "    def sortThird(val): \n",
        "      return val[2] \n",
        "    self.total_school_weight()\n",
        "    list1 =  [(fish.nb_convpool_blocks,fish.nb_fc_layers,fish.acc) for fish in self.school ]\n",
        "    list1.sort(key = sortThird, reverse = True)    \n",
        "    averge_convpool_blocks = np.mean([list1[i][0] for i in range (len(list1)//2)])\n",
        "    print(\"Average convpool\",averge_convpool_blocks) \n",
        "    average_fc_layers = np.mean([list1[i][1] for i in range (len(list1)//2)]) \n",
        "    print(\"Average fc\",average_fc_layers) \n",
        "    for fish in self.school:\n",
        "      #ecrire dans le fichier\n",
        "      if self.curr_weight_school > self.prev_weight_school:\n",
        "        if fish.nb_convpool_blocks > averge_convpool_blocks:\n",
        "          nb_convpool_blocks = -1 * abs((averge_convpool_blocks - fish.nb_convpool_blocks)) * 0.5 \n",
        "        else:\n",
        "          nb_convpool_blocks = abs((averge_convpool_blocks - fish.nb_convpool_blocks)) * 0.5 \n",
        "        if fish.nb_fc_layers > average_fc_layers:\n",
        "          nb_fc_layers =  -1 * abs((average_fc_layers - fish.nb_fc_layers)) * 0.5 \n",
        "        else:\n",
        "          nb_fc_layers =  abs((average_fc_layers - fish.nb_fc_layers)) * 0.5 \n",
        "      else:\n",
        "        if fish.nb_convpool_blocks > averge_convpool_blocks:\n",
        "          nb_convpool_blocks = abs((averge_convpool_blocks - fish.nb_convpool_blocks)) * 0.5 \n",
        "        else:\n",
        "          nb_convpool_blocks = -1* abs((averge_convpool_blocks - fish.nb_convpool_blocks)) * 0.5 \n",
        "        if fish.nb_fc_layers > average_fc_layers:\n",
        "          nb_fc_layers =  abs((average_fc_layers - fish.nb_fc_layers)) * 0.5 \n",
        "        else:\n",
        "          nb_fc_layers = -1 * abs((average_fc_layers - fish.nb_fc_layers)) * 0.5 \n",
        "      nb_convpool_blocks,nb_fc_layers = int(nb_convpool_blocks),int(nb_fc_layers)\n",
        "      '''nb_convpool_blocks = round((averge_convpool_blocks -  fish.nb_convpool_blocks)/2)     \n",
        "      nb_fc_layers = round((average_fc_layers - fish.nb_fc_layers) / 2 )\n",
        "      #if nb_convpool_blocks + nb_fc_blocks > max_depth or nb_convpool_blocks + nb_fc_blocks < min_depth:\n",
        "        #continue;\n",
        "      #else:\n",
        "      if (nb_convpool_blocks,nb_fc_layers)!=(0,0):\n",
        "        fish.collective_movement(int(nb_convpool_blocks),int(nb_fc_layers))\n",
        "      else:\n",
        "        print(\"NULL instinctive_movement\")\n",
        "        #print(\"OKKKKKKKKK\",nb_convpool_blocks,nb_fc_layers)\n",
        "      #else:\n",
        "        #print(\"one of the dimension is negative!\",nb_convpool_blocks,nb_fc_layers)'''\n",
        "      if (nb_convpool_blocks,nb_fc_layers)!=(0,0):\n",
        "        print('\\033[95m'+\"collective movement steps:\"+'\\033[0m',int(nb_convpool_blocks),int(nb_fc_layers))\n",
        "        fish.collective_movement(int(nb_convpool_blocks),int(nb_fc_layers))\n",
        "      else:\n",
        "        print('\\033[91m'+\"null collective movement\"+'\\033[0m')\n",
        "  '''def collective_volitive_movement(self):\n",
        "    \n",
        "    self.total_school_weight()\n",
        "    barycenter = self.calculate_barycenter()\n",
        "    for fish in self.school:\n",
        "      if self.curr_weight_school > self.prev_weight_school:\n",
        "        if fish.nb_convpool_blocks > barycenter[0]:\n",
        "          nb_convpool_blocks = -1 * abs((barycenter[0] - fish.nb_convpool_blocks)) * 0.5 \n",
        "        else:\n",
        "          nb_convpool_blocks = abs((barycenter[0] - fish.nb_convpool_blocks)) * 0.5 \n",
        "        if fish.nb_fc_layers > barycenter[1]:\n",
        "          nb_fc_layers =  -1 * abs((barycenter[1] - fish.nb_fc_layers)) * 0.5 \n",
        "        else:\n",
        "          nb_fc_layers =  abs((barycenter[1] - fish.nb_fc_layers)) * 0.5 \n",
        "      else:\n",
        "        if fish.nb_convpool_blocks > barycenter[0]:\n",
        "          nb_convpool_blocks = abs((barycenter[0] - fish.nb_convpool_blocks)) * 0.5 \n",
        "        else:\n",
        "          nb_convpool_blocks = -1* abs((barycenter[0] - fish.nb_convpool_blocks)) * 0.5 \n",
        "        if fish.nb_fc_layers > barycenter[1]:\n",
        "          nb_fc_layers =  abs((barycenter[1] - fish.nb_fc_layers)) * 0.5 \n",
        "        else:\n",
        "          nb_fc_layers = -1 * abs((barycenter[1] - fish.nb_fc_layers)) * 0.5 \n",
        "      nb_convpool_blocks,nb_fc_layers = int(nb_convpool_blocks),int(nb_fc_layers)\n",
        "      print(\"Collective_volitive_movement:\",nb_convpool_blocks,nb_fc_layers)\n",
        "      if (nb_convpool_blocks,nb_fc_layers)!=(0,0):\n",
        "        fish.collective_movement(int(nb_convpool_blocks),int(nb_fc_layers))\n",
        "      else:\n",
        "        print(\"null volitive\")\n",
        "      #cost = self.objective_function.evaluate(new_pos)\n",
        "    #self.update_best_fish()'''\n",
        "\n",
        "  def optimize(self):\n",
        "    #self.__init__(max_num_words,embedding_dim,max_seq_length,max_kernel_size,nb_classes,\n",
        "               #min_depth, max_depth,conv_pool_prob, fc_prob, max_out_ch,max_fc_neurons,n_iter,school_size)\n",
        "    track_Best_fish=[]\n",
        "    track_fish_before_indiv_mvt=[]\n",
        "    track_fish_after_indiv_mvt=[]\n",
        "    track_fish_before_collec_mvt=[]\n",
        "    track_fish_before_collec_mvt=[]\n",
        "    \n",
        "    for i in tqdm(range(self.n_iter)):\n",
        "      find.write(\"Iteration number: \" + str(i)+'\\n')\n",
        "      fcoll.write(\"Iteration number: \" + str(i)+'\\n')\n",
        "      fiter.write(\"Iteration number: \" + str(i)+'\\n')\n",
        "      k=1\n",
        "      for fish in self.school:\n",
        "        find.write(\"fish number \"+str(k)+'before individual movement :'+str(fish.acc)+'---')\n",
        "        fish.individual_movement()\n",
        "        find.write('after individual movement :'+str(fish.acc)+'\\n')\n",
        "        k+=1\n",
        "      fiter.write('Best fish before individual movement:'+ str(self.best_fish_acc)+'----')\n",
        "      self.update_best_fish()\n",
        "      fiter.write('Best fish after individual movement:'+ str(self.best_fish_acc)+'\\n')\n",
        "      #self.feeding()\n",
        "      fcoll.write('Best fish before collective movement:'+ str(self.best_fish_acc)+'----')\n",
        "      self.collective_movement()\n",
        "      #self.collective_volitive_movement()\n",
        "      #self.update_steps(i)\n",
        "      self.update_best_fish()\n",
        "      fcoll.write('Best fish after collective movement:'+ str(self.best_fish_acc)+'\\n')\n",
        "      \n",
        "      print (\"Iteration: \", i, \" Cost: \", self.best_fish.acc)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPOfy9_G4mun"
      },
      "source": [
        "def create_dir(path):\n",
        "    directory = os.path.dirname(path)\n",
        "    try:\n",
        "        os.stat(directory)\n",
        "    except:\n",
        "        os.mkdir(directory)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3y35i8sDhMVo",
        "outputId": "7bbec4f4-3bda-4a77-8a3f-bf849f07cd07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "max_num_words= MAX_NUM_WORDS\n",
        "embedding_dim=EMBEDDING_DIM\n",
        "max_seq_length=MAX_SEQ_LENGTH\n",
        "max_kernel_size=7\n",
        "nb_classes=10\n",
        "\n",
        "min_depth=2\n",
        "max_depth=3\n",
        "# Probability of each layer type (should sum to 1)\n",
        "conv_pool_prob=0.5\n",
        "fc_prob=0.5\n",
        "\n",
        "max_out_ch= 256\n",
        "max_fc_neurons= 300\n",
        "\n",
        "drop_rate=0.5\n",
        "batch_size=BATCH_SIZE\n",
        "nb_epochs = NB_EPOCHS\n",
        "epochs_full_training = 1\n",
        "\n",
        "runs_time=[]\n",
        "# FSS parameters\n",
        "results_path = \"Executions/\"\n",
        "num_runs = 1\n",
        "school_size = 20\n",
        "num_iterations = 2\n",
        "all_gbest_par=[]\n",
        "create_dir(results_path)\n",
        "find=open(results_path+'/individual_movement.txt', 'w')\n",
        "find.write(\"individual movement track\\n\")\n",
        "fcoll=open(results_path+'/collective_movement.txt', 'w')\n",
        "fcoll.write(\"collective movement track\\n\")\n",
        "fbest=open(results_path+'/Best_fish.txt', 'w')\n",
        "fbest.write(\"global best fish track\\n\")\n",
        "fiter=open(results_path+'/best_fish_OverIterations.txt', 'w')\n",
        "fiter.write(\"Best fish Over Iterations track\\n\")\n",
        "      \n",
        "for i in range (num_runs):\n",
        "  print(\"******************Run number: \" + str(i)+'************************\\n')\n",
        "  fbest.write(\"******************Run number: \" + str(i)+'************************\\n')\n",
        "  find.write(\"******************Run number: \" + str(i)+'************************\\n')\n",
        "  fcoll.write(\"******************Run number: \" + str(i)+'************************\\n')\n",
        "  fiter.write(\"******************Run number: \" + str(i)+'************************\\n')\n",
        "  start_time = time.time()\n",
        "  s = School(num_iterations,school_size)\n",
        "  s.optimize()\n",
        "  fbest.write(\"Best fish accuracy: \" + str(s.best_fish_acc)+'\\n')\n",
        "  print(s.best_fish_acc)\n",
        "  # Plot current Best_fish\n",
        "  matplotlib.use('Agg')\n",
        "  plt.plot(s.best_fish_acc)\n",
        "  plt.xlabel(\"Run\")\n",
        "  plt.ylabel(\"Fish Best accuracy\")\n",
        "  plt.savefig(results_path + \"gBest-iter-\" + str(i) + \".png\")\n",
        "  plt.close()\n",
        "\n",
        "  print('Best architecture found: ')\n",
        "  print(s.best_fish)\n",
        "  fbest.write(\"Best fish architecture found: \" + s.best_fish.__str__() +'\\n')\n",
        "  \n",
        "  #fbest.write(\"Parameters of Best fish architecture found: \" + s.best_fish.layer'\\n')  \n",
        "  np.save(results_path + \"Best_inter_\" + str(i) + \"_acc_history.npy\", s.best_fish_acc)\n",
        "\n",
        "  #np.save(results_path + \"gBest_iter_\" + str(i) + \"_test_acc_history.npy\", s.gBest_test_acc)\n",
        "\n",
        "  end_time = time.time()\n",
        "\n",
        "  running_time = end_time - start_time\n",
        "  fbest.write(\"Running Time: \" + str(running_time) +'\\n')\n",
        "  runs_time.append(running_time)\n",
        "\n",
        "  # Fully train and evaluate the Best model found\n",
        "   \n",
        "  n_parameters = model_fit(s.best_fish.model,X_train_fss, y_train_fss, batch_size,epochs_full_training, full_training = True)\n",
        "  #n_parameters = model_fit(s.best_fish.model,x_train, y_train, batch_size,epochs_full_training, full_training = True)\n",
        " \n",
        "  all_gbest_par.append(n_parameters)\n",
        "  print(all_gbest_par)\n",
        "  fbest.write(\"final model evaluation: \" + '\\n')\n",
        "  l1=map(lambda x:str(x)+';', all_gbest_par)\n",
        "  fbest.writelines(l1)\n",
        "  para=[]\n",
        "  for nn in s.best_fish.layers:\n",
        "    para=para+list(nn.values())\n",
        "  l1=map(lambda x:str(x)+';', para)\n",
        "  fbest.writelines(l1)\n",
        "  # Evaluate the fully trained gBest model\n",
        "   #gBest_metrics = pso.evaluate_gBest(batch_size=batch_size_full_training)\n",
        "find.close()\n",
        "fcoll.close()\n",
        "fiter.close()\n",
        "fbest.close()\n",
        "        \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "******************Run number: 0************************\n",
            "\n",
            "cnvpool= 1\n",
            "fc= 1\n",
            "[{'type': 'convpool_block', 'ou_c': 512, 'kernel': [3, 3, 3]}, {'type': 'fc', 'ou_c': 169, 'kernel': -1}]\n",
            "ok1\n",
            "ok2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 2s 2ms/step - loss: 2.3187 - acc: 0.1110\n",
            "1000/1000 [==============================] - 0s 465us/step\n",
            "ok3\n",
            "cnvpool= 1\n",
            "fc= 1\n",
            "[{'type': 'convpool_block', 'ou_c': 64, 'kernel': [2, 3, 4, 5]}, {'type': 'fc', 'ou_c': 111, 'kernel': -1}]\n",
            "ok1\n",
            "ok2\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 1s 909us/step - loss: 2.3105 - acc: 0.1060\n",
            "1000/1000 [==============================] - 0s 314us/step\n",
            "ok3\n",
            "cnvpool= 1\n",
            "fc= 1\n",
            "[{'type': 'convpool_block', 'ou_c': 128, 'kernel': [2, 3, 4]}, {'type': 'fc', 'ou_c': 127, 'kernel': -1}]\n",
            "ok1\n",
            "ok2\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 1s 873us/step - loss: 2.3121 - acc: 0.1150\n",
            "1000/1000 [==============================] - 0s 305us/step\n",
            "ok3\n",
            "cnvpool= 1\n",
            "fc= 1\n",
            "[{'type': 'convpool_block', 'ou_c': 128, 'kernel': [5, 5, 5, 5]}, {'type': 'fc', 'ou_c': 214, 'kernel': -1}]\n",
            "ok1\n",
            "ok2\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 1s 1ms/step - loss: 2.3152 - acc: 0.1070\n",
            "1000/1000 [==============================] - 0s 420us/step\n",
            "ok3\n",
            "cnvpool= 1\n",
            "fc= 1\n",
            "[{'type': 'convpool_block', 'ou_c': 512, 'kernel': [2, 3, 4]}, {'type': 'fc', 'ou_c': 150, 'kernel': -1}]\n",
            "ok1\n",
            "ok2\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 2s 2ms/step - loss: 2.3180 - acc: 0.0970\n",
            "1000/1000 [==============================] - 0s 490us/step\n",
            "ok3\n",
            "cnvpool= 1\n",
            "fc= 1\n",
            "[{'type': 'convpool_block', 'ou_c': 512, 'kernel': [5, 6, 7]}, {'type': 'fc', 'ou_c': 111, 'kernel': -1}]\n",
            "ok1\n",
            "ok2\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 2s 2ms/step - loss: 2.3108 - acc: 0.1010\n",
            "1000/1000 [==============================] - 1s 601us/step\n",
            "ok3\n",
            "cnvpool= 1\n",
            "fc= 1\n",
            "[{'type': 'convpool_block', 'ou_c': 512, 'kernel': [6, 7, 8]}, {'type': 'fc', 'ou_c': 123, 'kernel': -1}]\n",
            "ok1\n",
            "ok2\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 2s 2ms/step - loss: 2.3165 - acc: 0.1030\n",
            "1000/1000 [==============================] - 1s 603us/step\n",
            "ok3\n",
            "cnvpool= 1\n",
            "fc= 1\n",
            "[{'type': 'convpool_block', 'ou_c': 128, 'kernel': [6, 6, 6, 6]}, {'type': 'fc', 'ou_c': 125, 'kernel': -1}]\n",
            "ok1\n",
            "ok2\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 1s 1ms/step - loss: 2.3190 - acc: 0.1000\n",
            "1000/1000 [==============================] - 0s 436us/step\n",
            "ok3\n",
            "cnvpool= 1\n",
            "fc= 1\n",
            "[{'type': 'convpool_block', 'ou_c': 128, 'kernel': [5, 5, 5]}, {'type': 'fc', 'ou_c': 213, 'kernel': -1}]\n",
            "ok1\n",
            "ok2\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 1s 1ms/step - loss: 2.3126 - acc: 0.1000\n",
            "1000/1000 [==============================] - 0s 334us/step\n",
            "ok3\n",
            "cnvpool= 1\n",
            "fc= 1\n",
            "[{'type': 'convpool_block', 'ou_c': 128, 'kernel': [7, 7, 7]}, {'type': 'fc', 'ou_c': 182, 'kernel': -1}]\n",
            "ok1\n",
            "ok2\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 1s 1ms/step - loss: 2.3076 - acc: 0.1130\n",
            "1000/1000 [==============================] - 0s 344us/step\n",
            "ok3\n",
            "cnvpool= 1\n",
            "fc= 1\n",
            "[{'type': 'convpool_block', 'ou_c': 512, 'kernel': [4, 5, 6, 7]}, {'type': 'fc', 'ou_c': 130, 'kernel': -1}]\n",
            "ok1\n",
            "ok2\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 3s 3ms/step - loss: 2.3185 - acc: 0.1080\n",
            "1000/1000 [==============================] - 1s 729us/step\n",
            "ok3\n",
            "cnvpool= 1\n",
            "fc= 1\n",
            "[{'type': 'convpool_block', 'ou_c': 512, 'kernel': [6, 6, 6]}, {'type': 'fc', 'ou_c': 129, 'kernel': -1}]\n",
            "ok1\n",
            "ok2\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 2s 2ms/step - loss: 2.3160 - acc: 0.0980\n",
            "1000/1000 [==============================] - 1s 615us/step\n",
            "ok3\n",
            "cnvpool= 1\n",
            "fc= 1\n",
            "[{'type': 'convpool_block', 'ou_c': 64, 'kernel': [6, 7, 8]}, {'type': 'fc', 'ou_c': 260, 'kernel': -1}]\n",
            "ok1\n",
            "ok2\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 1s 882us/step - loss: 2.3024 - acc: 0.1180\n",
            "1000/1000 [==============================] - 0s 307us/step\n",
            "ok3\n",
            "cnvpool= 1\n",
            "fc= 1\n",
            "[{'type': 'convpool_block', 'ou_c': 64, 'kernel': [7, 7, 7]}, {'type': 'fc', 'ou_c': 103, 'kernel': -1}]\n",
            "ok1\n",
            "ok2\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 1s 843us/step - loss: 2.2974 - acc: 0.1200\n",
            "1000/1000 [==============================] - 0s 288us/step\n",
            "ok3\n",
            "cnvpool= 1\n",
            "fc= 1\n",
            "[{'type': 'convpool_block', 'ou_c': 64, 'kernel': [5, 6, 7, 8]}, {'type': 'fc', 'ou_c': 218, 'kernel': -1}]\n",
            "ok1\n",
            "ok2\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 1s 1ms/step - loss: 2.3153 - acc: 0.0940\n",
            "1000/1000 [==============================] - 0s 342us/step\n",
            "ok3\n",
            "cnvpool= 1\n",
            "fc= 1\n",
            "[{'type': 'convpool_block', 'ou_c': 64, 'kernel': [7, 8, 9]}, {'type': 'fc', 'ou_c': 150, 'kernel': -1}]\n",
            "ok1\n",
            "ok2\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 1s 867us/step - loss: 2.3102 - acc: 0.1010\n",
            "1000/1000 [==============================] - 0s 296us/step\n",
            "ok3\n",
            "cnvpool= 1\n",
            "fc= 1\n",
            "[{'type': 'convpool_block', 'ou_c': 64, 'kernel': [3, 4, 5, 6]}, {'type': 'fc', 'ou_c': 140, 'kernel': -1}]\n",
            "ok1\n",
            "ok2\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 1s 917us/step - loss: 2.3084 - acc: 0.0970\n",
            "1000/1000 [==============================] - 0s 326us/step\n",
            "ok3\n",
            "cnvpool= 1\n",
            "fc= 1\n",
            "[{'type': 'convpool_block', 'ou_c': 512, 'kernel': [6, 6, 6, 6]}, {'type': 'fc', 'ou_c': 291, 'kernel': -1}]\n",
            "ok1\n",
            "ok2\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 3s 3ms/step - loss: 2.3244 - acc: 0.1160\n",
            "1000/1000 [==============================] - 1s 769us/step\n",
            "ok3\n",
            "cnvpool= 1\n",
            "fc= 1\n",
            "[{'type': 'convpool_block', 'ou_c': 64, 'kernel': [4, 4, 4, 4]}, {'type': 'fc', 'ou_c': 230, 'kernel': -1}]\n",
            "ok1\n",
            "ok2\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 1s 936us/step - loss: 2.3174 - acc: 0.0920\n",
            "1000/1000 [==============================] - 0s 331us/step\n",
            "ok3\n",
            "cnvpool= 1\n",
            "fc= 1\n",
            "[{'type': 'convpool_block', 'ou_c': 128, 'kernel': [7, 7, 7]}, {'type': 'fc', 'ou_c': 235, 'kernel': -1}]\n",
            "ok1\n",
            "ok2\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 1s 1ms/step - loss: 2.3131 - acc: 0.1120\n",
            "1000/1000 [==============================] - 0s 330us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/2 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ok3\n",
            "Starting individual movement\n",
            "initial fish accuracy: 0.18299999833106995\n",
            "try number: 0\n",
            "Steps: -2 1\n",
            "compiling model\n",
            "training model\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 2s 2ms/step - loss: 2.3159 - acc: 0.1010\n",
            "1000/1000 [==============================] - 0s 475us/step\n",
            "Fish accuracy after modification 0.11999999731779099\n",
            "Improvement after modification:\u001b[91mNO\u001b[0m\n",
            "Starting individual movement\n",
            "initial fish accuracy: 0.10499999672174454\n",
            "try number: 0\n",
            "Steps: 1 1\n",
            "compiling model\n",
            "training model\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 3s 3ms/step - loss: 2.3201 - acc: 0.1000\n",
            "1000/1000 [==============================] - 1s 736us/step\n",
            "Fish accuracy after modification 0.11100000143051147\n",
            "Improvement after modification:\u001b[92mYES\u001b[0m\n",
            "Starting individual movement\n",
            "initial fish accuracy: 0.11999999731779099\n",
            "try number: 0\n",
            "Steps: 1 -2\n",
            "compiling model\n",
            "training model\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 2s 2ms/step - loss: 2.3151 - acc: 0.1080\n",
            "1000/1000 [==============================] - 1s 524us/step\n",
            "Fish accuracy after modification 0.10499999672174454\n",
            "Improvement after modification:\u001b[91mNO\u001b[0m\n",
            "Starting individual movement\n",
            "initial fish accuracy: 0.11100000143051147\n",
            "try number: 0\n",
            "Steps: -1 0\n",
            "compiling model\n",
            "training model\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 1s 1ms/step - loss: 2.3130 - acc: 0.1070\n",
            "1000/1000 [==============================] - 0s 400us/step\n",
            "Fish accuracy after modification 0.12800000607967377\n",
            "Improvement after modification:\u001b[92mYES\u001b[0m\n",
            "Starting individual movement\n",
            "initial fish accuracy: 0.1340000033378601\n",
            "try number: 0\n",
            "Steps: 2 2\n",
            "compiling model\n",
            "training model\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 1s 756us/step - loss: 2.3062 - acc: 0.1180\n",
            "1000/1000 [==============================] - 0s 272us/step\n",
            "Fish accuracy after modification 0.10999999940395355\n",
            "Improvement after modification:\u001b[91mNO\u001b[0m\n",
            "Starting individual movement\n",
            "initial fish accuracy: 0.14900000393390656\n",
            "try number: 0\n",
            "Steps: 1 1\n",
            "compiling model\n",
            "training model\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 1s 1ms/step - loss: 2.3183 - acc: 0.0970\n",
            "1000/1000 [==============================] - 0s 408us/step\n",
            "Fish accuracy after modification 0.1080000028014183\n",
            "Improvement after modification:\u001b[91mNO\u001b[0m\n",
            "Starting individual movement\n",
            "initial fish accuracy: 0.15600000321865082\n",
            "try number: 0\n",
            "Steps: 1 0\n",
            "compiling model\n",
            "training model\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 3s 3ms/step - loss: 2.3218 - acc: 0.1230\n",
            "1000/1000 [==============================] - 1s 733us/step\n",
            "Fish accuracy after modification 0.13199999928474426\n",
            "Improvement after modification:\u001b[91mNO\u001b[0m\n",
            "Starting individual movement\n",
            "initial fish accuracy: 0.10999999940395355\n",
            "try number: 0\n",
            "Steps: 1 1\n",
            "compiling model\n",
            "training model\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 3s 3ms/step - loss: 2.3245 - acc: 0.1040\n",
            "1000/1000 [==============================] - 1s 753us/step\n",
            "Fish accuracy after modification 0.13899999856948853\n",
            "Improvement after modification:\u001b[92mYES\u001b[0m\n",
            "Starting individual movement\n",
            "initial fish accuracy: 0.11100000143051147\n",
            "try number: 0\n",
            "Steps: 0 0\n",
            "compiling model\n",
            "training model\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 1s 962us/step - loss: 2.3131 - acc: 0.1000\n",
            "1000/1000 [==============================] - 0s 339us/step\n",
            "Fish accuracy after modification 0.10700000077486038\n",
            "Improvement after modification:\u001b[91mNO\u001b[0m\n",
            "Starting individual movement\n",
            "initial fish accuracy: 0.12300000339746475\n",
            "try number: 0\n",
            "Steps: -1 2\n",
            "compiling model\n",
            "training model\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 1s 1ms/step - loss: 2.3096 - acc: 0.1080\n",
            "1000/1000 [==============================] - 0s 335us/step\n",
            "Fish accuracy after modification 0.12099999934434891\n",
            "Improvement after modification:\u001b[91mNO\u001b[0m\n",
            "Starting individual movement\n",
            "initial fish accuracy: 0.13600000739097595\n",
            "try number: 0\n",
            "Steps: 2 0\n",
            "compiling model\n",
            "training model\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 1s 1ms/step - loss: 2.3087 - acc: 0.1100\n",
            "1000/1000 [==============================] - 0s 387us/step\n",
            "Fish accuracy after modification 0.13699999451637268\n",
            "Improvement after modification:\u001b[92mYES\u001b[0m\n",
            "Starting individual movement\n",
            "initial fish accuracy: 0.1080000028014183\n",
            "try number: 0\n",
            "Steps: 1 -2\n",
            "compiling model\n",
            "training model\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 1s 1ms/step - loss: 2.3152 - acc: 0.1010\n",
            "1000/1000 [==============================] - 0s 347us/step\n",
            "Fish accuracy after modification 0.12099999934434891\n",
            "Improvement after modification:\u001b[92mYES\u001b[0m\n",
            "Starting individual movement\n",
            "initial fish accuracy: 0.12999999523162842\n",
            "try number: 0\n",
            "Steps: -2 0\n",
            "compiling model\n",
            "training model\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 1s 869us/step - loss: 2.3046 - acc: 0.1130\n",
            "1000/1000 [==============================] - 0s 304us/step\n",
            "Fish accuracy after modification 0.10999999940395355\n",
            "Improvement after modification:\u001b[91mNO\u001b[0m\n",
            "Starting individual movement\n",
            "initial fish accuracy: 0.12700000405311584\n",
            "try number: 0\n",
            "Steps: -2 1\n",
            "compiling model\n",
            "training model\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 1s 845us/step - loss: 2.3112 - acc: 0.0960\n",
            "1000/1000 [==============================] - 0s 295us/step\n",
            "Fish accuracy after modification 0.11999999731779099\n",
            "Improvement after modification:\u001b[91mNO\u001b[0m\n",
            "Starting individual movement\n",
            "initial fish accuracy: 0.10700000077486038\n",
            "try number: 0\n",
            "Steps: -1 -1\n",
            "compiling model\n",
            "training model\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 1s 1ms/step - loss: 2.3152 - acc: 0.1060\n",
            "1000/1000 [==============================] - 0s 354us/step\n",
            "Fish accuracy after modification 0.11999999731779099\n",
            "Improvement after modification:\u001b[92mYES\u001b[0m\n",
            "Starting individual movement\n",
            "initial fish accuracy: 0.1080000028014183\n",
            "try number: 0\n",
            "Steps: 1 2\n",
            "compiling model\n",
            "training model\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 1s 882us/step - loss: 2.3089 - acc: 0.1010\n",
            "1000/1000 [==============================] - 0s 306us/step\n",
            "Fish accuracy after modification 0.11299999803304672\n",
            "Improvement after modification:\u001b[92mYES\u001b[0m\n",
            "Starting individual movement\n",
            "initial fish accuracy: 0.13099999725818634\n",
            "try number: 0\n",
            "Steps: -1 -1\n",
            "compiling model\n",
            "training model\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 1s 973us/step - loss: 2.3105 - acc: 0.1130\n",
            "1000/1000 [==============================] - 0s 334us/step\n",
            "Fish accuracy after modification 0.10599999874830246\n",
            "Improvement after modification:\u001b[91mNO\u001b[0m\n",
            "Starting individual movement\n",
            "initial fish accuracy: 0.10599999874830246\n",
            "try number: 0\n",
            "Steps: -1 2\n",
            "compiling model\n",
            "training model\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 3s 3ms/step - loss: 2.3362 - acc: 0.0970\n",
            "1000/1000 [==============================] - 1s 767us/step\n",
            "Fish accuracy after modification 0.11999999731779099\n",
            "Improvement after modification:\u001b[92mYES\u001b[0m\n",
            "Starting individual movement\n",
            "initial fish accuracy: 0.1289999932050705\n",
            "try number: 0\n",
            "Steps: 2 1\n",
            "compiling model\n",
            "training model\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 1s 1ms/step - loss: 2.3128 - acc: 0.1050\n",
            "1000/1000 [==============================] - 0s 447us/step\n",
            "Fish accuracy after modification 0.08299999684095383\n",
            "Improvement after modification:\u001b[91mNO\u001b[0m\n",
            "Starting individual movement\n",
            "initial fish accuracy: 0.11299999803304672\n",
            "try number: 0\n",
            "Steps: 0 2\n",
            "compiling model\n",
            "training model\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 1s 1ms/step - loss: 2.3004 - acc: 0.1070\n",
            "1000/1000 [==============================] - 0s 333us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 1/2 [01:09<01:09, 69.16s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fish accuracy after modification 0.10999999940395355\n",
            "Improvement after modification:\u001b[91mNO\u001b[0m\n",
            "Average convpool 1.3\n",
            "Average fc 1.1\n",
            "\u001b[91mnull collective movement\u001b[0m\n",
            "\u001b[91mnull collective movement\u001b[0m\n",
            "\u001b[91mnull collective movement\u001b[0m\n",
            "\u001b[91mnull collective movement\u001b[0m\n",
            "\u001b[91mnull collective movement\u001b[0m\n",
            "\u001b[91mnull collective movement\u001b[0m\n",
            "\u001b[91mnull collective movement\u001b[0m\n",
            "\u001b[91mnull collective movement\u001b[0m\n",
            "\u001b[91mnull collective movement\u001b[0m\n",
            "\u001b[91mnull collective movement\u001b[0m\n",
            "\u001b[91mnull collective movement\u001b[0m\n",
            "\u001b[91mnull collective movement\u001b[0m\n",
            "\u001b[91mnull collective movement\u001b[0m\n",
            "\u001b[91mnull collective movement\u001b[0m\n",
            "\u001b[91mnull collective movement\u001b[0m\n",
            "\u001b[91mnull collective movement\u001b[0m\n",
            "\u001b[91mnull collective movement\u001b[0m\n",
            "\u001b[91mnull collective movement\u001b[0m\n",
            "\u001b[91mnull collective movement\u001b[0m\n",
            "\u001b[91mnull collective movement\u001b[0m\n",
            "Iteration:  0  Cost:  0.18299999833106995\n",
            "Starting individual movement\n",
            "initial fish accuracy: 0.18299999833106995\n",
            "try number: 0\n",
            "Steps: 1 -1\n",
            "compiling model\n",
            "training model\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 1s 858us/step - loss: 2.3066 - acc: 0.1060\n",
            "1000/1000 [==============================] - 0s 301us/step\n",
            "Fish accuracy after modification 0.1080000028014183\n",
            "Improvement after modification:\u001b[91mNO\u001b[0m\n",
            "Starting individual movement\n",
            "initial fish accuracy: 0.11100000143051147\n",
            "try number: 0\n",
            "Steps: -1 0\n",
            "compiling model\n",
            "training model\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 1s 987us/step - loss: 2.3161 - acc: 0.0990\n",
            "1000/1000 [==============================] - 0s 331us/step\n",
            "Fish accuracy after modification 0.10700000077486038\n",
            "Improvement after modification:\u001b[91mNO\u001b[0m\n",
            "Starting individual movement\n",
            "initial fish accuracy: 0.11999999731779099\n",
            "try number: 0\n",
            "Steps: 0 2\n",
            "compiling model\n",
            "training model\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 1s 903us/step - loss: 2.3030 - acc: 0.1200\n",
            "1000/1000 [==============================] - 0s 322us/step\n",
            "Fish accuracy after modification 0.1080000028014183\n",
            "Improvement after modification:\u001b[91mNO\u001b[0m\n",
            "Starting individual movement\n",
            "initial fish accuracy: 0.12800000607967377\n",
            "try number: 0\n",
            "Steps: 0 -2\n",
            "compiling model\n",
            "training model\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 1s 1ms/step - loss: 2.3137 - acc: 0.1100\n",
            "1000/1000 [==============================] - 0s 418us/step\n",
            "Fish accuracy after modification 0.08900000154972076\n",
            "Improvement after modification:\u001b[91mNO\u001b[0m\n",
            "Starting individual movement\n",
            "initial fish accuracy: 0.1340000033378601\n",
            "try number: 0\n",
            "Steps: 0 1\n",
            "compiling model\n",
            "training model\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 2s 2ms/step - loss: 2.3090 - acc: 0.1160\n",
            "1000/1000 [==============================] - 0s 485us/step\n",
            "Fish accuracy after modification 0.11699999868869781\n",
            "Improvement after modification:\u001b[91mNO\u001b[0m\n",
            "Starting individual movement\n",
            "initial fish accuracy: 0.14900000393390656\n",
            "try number: 0\n",
            "Steps: 1 -2\n",
            "compiling model\n",
            "training model\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 2s 2ms/step - loss: 2.3129 - acc: 0.1090\n",
            "1000/1000 [==============================] - 1s 580us/step\n",
            "Fish accuracy after modification 0.10999999940395355\n",
            "Improvement after modification:\u001b[91mNO\u001b[0m\n",
            "Starting individual movement\n",
            "initial fish accuracy: 0.15600000321865082\n",
            "try number: 0\n",
            "Steps: -2 -1\n",
            "compiling model\n",
            "training model\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 2s 2ms/step - loss: 2.3092 - acc: 0.1180\n",
            "1000/1000 [==============================] - 1s 610us/step\n",
            "Fish accuracy after modification 0.11299999803304672\n",
            "Improvement after modification:\u001b[91mNO\u001b[0m\n",
            "Starting individual movement\n",
            "initial fish accuracy: 0.13899999856948853\n",
            "try number: 0\n",
            "Steps: 1 0\n",
            "compiling model\n",
            "training model\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 1s 1ms/step - loss: 2.3106 - acc: 0.1230\n",
            "1000/1000 [==============================] - 0s 355us/step\n",
            "Fish accuracy after modification 0.12099999934434891\n",
            "Improvement after modification:\u001b[91mNO\u001b[0m\n",
            "Starting individual movement\n",
            "initial fish accuracy: 0.11100000143051147\n",
            "try number: 0\n",
            "Steps: -2 0\n",
            "compiling model\n",
            "training model\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 1s 990us/step - loss: 2.3205 - acc: 0.0940\n",
            "1000/1000 [==============================] - 0s 333us/step\n",
            "Fish accuracy after modification 0.11500000208616257\n",
            "Improvement after modification:\u001b[92mYES\u001b[0m\n",
            "Starting individual movement\n",
            "initial fish accuracy: 0.12300000339746475\n",
            "try number: 0\n",
            "Steps: 1 0\n",
            "compiling model\n",
            "training model\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 1s 1ms/step - loss: 2.3097 - acc: 0.1120\n",
            "1000/1000 [==============================] - 0s 383us/step\n",
            "Fish accuracy after modification 0.10700000077486038\n",
            "Improvement after modification:\u001b[91mNO\u001b[0m\n",
            "Starting individual movement\n",
            "initial fish accuracy: 0.13699999451637268\n",
            "try number: 0\n",
            "Steps: -1 2\n",
            "compiling model\n",
            "training model\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 1s 1ms/step - loss: 2.3184 - acc: 0.0970\n",
            "1000/1000 [==============================] - 0s 408us/step\n",
            "Fish accuracy after modification 0.13099999725818634\n",
            "Improvement after modification:\u001b[91mNO\u001b[0m\n",
            "Starting individual movement\n",
            "initial fish accuracy: 0.12099999934434891\n",
            "try number: 0\n",
            "Steps: 1 -2\n",
            "compiling model\n",
            "training model\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 1s 1ms/step - loss: 2.3084 - acc: 0.1130\n",
            "1000/1000 [==============================] - 0s 350us/step\n",
            "Fish accuracy after modification 0.15199999511241913\n",
            "Improvement after modification:\u001b[92mYES\u001b[0m\n",
            "Starting individual movement\n",
            "initial fish accuracy: 0.12999999523162842\n",
            "try number: 0\n",
            "Steps: 1 2\n",
            "compiling model\n",
            "training model\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 1s 935us/step - loss: 2.3145 - acc: 0.0970\n",
            "1000/1000 [==============================] - 0s 323us/step\n",
            "Fish accuracy after modification 0.10999999940395355\n",
            "Improvement after modification:\u001b[91mNO\u001b[0m\n",
            "Starting individual movement\n",
            "initial fish accuracy: 0.12700000405311584\n",
            "try number: 0\n",
            "Steps: 0 2\n",
            "compiling model\n",
            "training model\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 1s 943us/step - loss: 2.3072 - acc: 0.1060\n",
            "1000/1000 [==============================] - 0s 293us/step\n",
            "Fish accuracy after modification 0.10899999737739563\n",
            "Improvement after modification:\u001b[91mNO\u001b[0m\n",
            "Starting individual movement\n",
            "initial fish accuracy: 0.11999999731779099\n",
            "try number: 0\n",
            "Steps: -1 -1\n",
            "compiling model\n",
            "training model\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 1s 1ms/step - loss: 2.3110 - acc: 0.1100\n",
            "1000/1000 [==============================] - 0s 378us/step\n",
            "Fish accuracy after modification 0.11699999868869781\n",
            "Improvement after modification:\u001b[91mNO\u001b[0m\n",
            "Starting individual movement\n",
            "initial fish accuracy: 0.11299999803304672\n",
            "try number: 0\n",
            "Steps: -1 -1\n",
            "architecture before modification: [{'type': 'convpool_block', 'ou_c': 64, 'kernel': [7, 8, 9]}, {'type': 'fc', 'ou_c': 76, 'kernel': -1}, {'type': 'fc', 'ou_c': 216, 'kernel': -1}, {'type': 'fc', 'ou_c': 150, 'kernel': -1}]\n",
            "compiling model\n",
            "training model\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 1s 910us/step - loss: 2.3097 - acc: 0.1000\n",
            "1000/1000 [==============================] - 0s 307us/step\n",
            "Fish accuracy after modification 0.11900000274181366\n",
            "Improvement after modification:\u001b[92mYES\u001b[0m\n",
            "Starting individual movement\n",
            "initial fish accuracy: 0.13099999725818634\n",
            "try number: 0\n",
            "Steps: 0 -2\n",
            "compiling model\n",
            "training model\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 1s 966us/step - loss: 2.3066 - acc: 0.1050\n",
            "1000/1000 [==============================] - 0s 342us/step\n",
            "Fish accuracy after modification 0.11400000005960464\n",
            "Improvement after modification:\u001b[91mNO\u001b[0m\n",
            "Starting individual movement\n",
            "initial fish accuracy: 0.11999999731779099\n",
            "try number: 0\n",
            "Steps: -2 1\n",
            "compiling model\n",
            "training model\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 3s 3ms/step - loss: 2.3417 - acc: 0.1080\n",
            "1000/1000 [==============================] - 1s 763us/step\n",
            "Fish accuracy after modification 0.10700000077486038\n",
            "Improvement after modification:\u001b[91mNO\u001b[0m\n",
            "Starting individual movement\n",
            "initial fish accuracy: 0.1289999932050705\n",
            "try number: 0\n",
            "Steps: 1 2\n",
            "compiling model\n",
            "training model\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 1s 890us/step - loss: 2.3050 - acc: 0.1160\n",
            "1000/1000 [==============================] - 0s 306us/step\n",
            "Fish accuracy after modification 0.11500000208616257\n",
            "Improvement after modification:\u001b[91mNO\u001b[0m\n",
            "Starting individual movement\n",
            "initial fish accuracy: 0.11299999803304672\n",
            "try number: 0\n",
            "Steps: 2 0\n",
            "compiling model\n",
            "training model\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 1s 1ms/step - loss: 2.3071 - acc: 0.1080\n",
            "1000/1000 [==============================] - 0s 342us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [02:12<00:00, 66.31s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fish accuracy after modification 0.10000000149011612\n",
            "Improvement after modification:\u001b[91mNO\u001b[0m\n",
            "Average convpool 1.5\n",
            "Average fc 1.1\n",
            "\u001b[91mnull collective movement\u001b[0m\n",
            "\u001b[91mnull collective movement\u001b[0m\n",
            "\u001b[91mnull collective movement\u001b[0m\n",
            "\u001b[91mnull collective movement\u001b[0m\n",
            "\u001b[91mnull collective movement\u001b[0m\n",
            "\u001b[91mnull collective movement\u001b[0m\n",
            "\u001b[91mnull collective movement\u001b[0m\n",
            "\u001b[91mnull collective movement\u001b[0m\n",
            "\u001b[91mnull collective movement\u001b[0m\n",
            "\u001b[91mnull collective movement\u001b[0m\n",
            "\u001b[91mnull collective movement\u001b[0m\n",
            "\u001b[91mnull collective movement\u001b[0m\n",
            "\u001b[91mnull collective movement\u001b[0m\n",
            "\u001b[91mnull collective movement\u001b[0m\n",
            "\u001b[91mnull collective movement\u001b[0m\n",
            "\u001b[91mnull collective movement\u001b[0m\n",
            "\u001b[91mnull collective movement\u001b[0m\n",
            "\u001b[91mnull collective movement\u001b[0m\n",
            "\u001b[91mnull collective movement\u001b[0m\n",
            "\u001b[91mnull collective movement\u001b[0m\n",
            "Iteration:  1  Cost:  0.18299999833106995\n",
            "0.18299999833106995\n",
            "Best architecture found: \n",
            "\u001b[94m\u001b[1mConv_layer || MaxPoling_layer || FullyConnected_layer || \u001b[0m\n",
            "Model: \"model_123\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_123 (InputLayer)          (None, 512)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_123 (Embedding)       (None, 512, 300)     1500000     input_123[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_123 (SpatialD (None, 512, 300)     0           embedding_123[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_424 (Conv1D)             (None, 510, 512)     461312      spatial_dropout1d_123[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_425 (Conv1D)             (None, 510, 512)     461312      spatial_dropout1d_123[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_426 (Conv1D)             (None, 510, 512)     461312      spatial_dropout1d_123[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_424 (MaxPooling1D (None, 1, 512)       0           conv1d_424[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_425 (MaxPooling1D (None, 1, 512)       0           conv1d_425[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_426 (MaxPooling1D (None, 1, 512)       0           conv1d_426[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "flatten_424 (Flatten)           (None, 512)          0           max_pooling1d_424[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "flatten_425 (Flatten)           (None, 512)          0           max_pooling1d_425[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "flatten_426 (Flatten)           (None, 512)          0           max_pooling1d_426[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_123 (Concatenate)   (None, 1536)         0           flatten_424[0][0]                \n",
            "                                                                 flatten_425[0][0]                \n",
            "                                                                 flatten_426[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_123 (Dropout)           (None, 1536)         0           concatenate_123[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_245 (Dense)               (None, 169)          259753      dropout_123[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_246 (Dense)               (None, 10)           1700        dense_245[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 3,145,389\n",
            "Trainable params: 3,145,389\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1397427/1397427 [==============================] - 1934s 1ms/step - loss: 1.3866 - acc: 0.5438\n",
            "62572/62572 [==============================] - 26s 409us/step\n",
            "[[1.323615030262969, 0.5628236532211304]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sxrvy3HGqHV",
        "outputId": "aef9824e-21a2-41d8-d4be-47c932966e5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "s.best_fish.model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_187\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_187 (InputLayer)          (None, 512)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_187 (Embedding)       (None, 512, 300)     1500000     input_187[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_187 (SpatialD (None, 512, 300)     0           embedding_187[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_667 (Conv1D)             (None, 508, 512)     768512      spatial_dropout1d_187[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_668 (Conv1D)             (None, 507, 512)     922112      spatial_dropout1d_187[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_669 (Conv1D)             (None, 506, 512)     1075712     spatial_dropout1d_187[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_667 (MaxPooling1D (None, 1, 512)       0           conv1d_667[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_668 (MaxPooling1D (None, 1, 512)       0           conv1d_668[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_669 (MaxPooling1D (None, 1, 512)       0           conv1d_669[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "flatten_667 (Flatten)           (None, 512)          0           max_pooling1d_667[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "flatten_668 (Flatten)           (None, 512)          0           max_pooling1d_668[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "flatten_669 (Flatten)           (None, 512)          0           max_pooling1d_669[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_187 (Concatenate)   (None, 1536)         0           flatten_667[0][0]                \n",
            "                                                                 flatten_668[0][0]                \n",
            "                                                                 flatten_669[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_187 (Dropout)           (None, 1536)         0           concatenate_187[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_373 (Dense)               (None, 192)          295104      dropout_187[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_374 (Dense)               (None, 5)            965         dense_373[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 4,562,405\n",
            "Trainable params: 4,562,405\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNf6KuowS2eB",
        "outputId": "cbf73d36-d0d0-47ab-919a-88bf2bcb7d46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "hist = s.best_fish.model.fit(x=x_train, y=y_train,batch_size=1, epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 1.0551 - acc: 0.5760\n",
            "Epoch 2/10\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 0.6431 - acc: 0.7440\n",
            "Epoch 3/10\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4348 - acc: 0.8360\n",
            "Epoch 4/10\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 0.3766 - acc: 0.8800\n",
            "Epoch 5/10\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 0.2207 - acc: 0.9350\n",
            "Epoch 6/10\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 0.1418 - acc: 0.9560\n",
            "Epoch 7/10\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 0.1401 - acc: 0.9540\n",
            "Epoch 8/10\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 0.1088 - acc: 0.9650\n",
            "Epoch 9/10\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 0.1105 - acc: 0.9630\n",
            "Epoch 10/10\n",
            "1000/1000 [==============================] - 9s 9ms/step - loss: 0.1872 - acc: 0.9520\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbeEnCZJEb-C",
        "outputId": "963c990a-4dd4-4c23-a36a-703a6fdc18a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "result=s.best_fish.model.evaluate(x_test,  y_test, verbose=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000/1000 [==============================] - 1s 517us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQwm-JzpdS4A",
        "outputId": "2bdf14d9-e98e-462c-ede8-5674df7936d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(hist.history.keys())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['loss', 'acc'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Fy5qj6OdsWw"
      },
      "source": [
        "plt.plot(hist.history['acc'])\n",
        "\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgh0wfIdiv-Z"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "am60GWOeNOVE"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
        "from sklearn import model_selection"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAuCA2vhDyqv",
        "outputId": "5a2b69d6-e454-4d52-9ae7-d3f8709357f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "hist.history['acc']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.576, 0.744, 0.836, 0.88, 0.935, 0.956, 0.954, 0.965, 0.963, 0.952]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxXCWVxfPtBX",
        "outputId": "2ea23eab-270e-45be-807c-39216249ec84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Compare Algorithms\n",
        "\n",
        "# evaluate each model in turn\n",
        "results = [hist.history['acc']]\n",
        "names = ['bbbb']\n",
        "\n",
        "# boxplot algorithm comparison\n",
        "plt.figure()\n",
        "fig.suptitle('Algorithm Comparison')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.boxplot(results)\n",
        "#ax.set_xticklabels(names)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDZt5GXkaPEh"
      },
      "source": [
        "acc_train = hist.history['acc']\n",
        "loss_train = hist.history['loss']\n",
        "\n",
        "#loss_val = history.history['val_acc']\n",
        "epochs = range(1,11)\n",
        "plt.plot(epochs, acc_train, 'g', label='Training accuracy')\n",
        "plt.plot(epochs, loss_train, 'b', label='validation accuracy')\n",
        "plt.title('Training and Validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BuunZ3FZ_TJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UZaaSbMPahB"
      },
      "source": [
        "# boxplot algorithm comparison\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Algorithm Comparison')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.boxplot(n_parameters)\n",
        "ax.set_xticklabels(['ff'])\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}